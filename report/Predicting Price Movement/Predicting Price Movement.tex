\chapter{Predicting Price Movement}
	\section{Technical Analysis}
		There are two main schools of thought in financial markets, technical analysis and fundamental analysis. Technical analysis concentrates on the study of market action by looking a the price previous price movement of a security, fundamental analysis focuses on the economic forces of supply and demand that cause prices to move higher, lower, or stay the same\cite{murphy1999technical}. Both approaches have the same desired outcome: to predict the direction of future price movement. Technical Analysis assumes that the economic forces of supply and demand are already implied by the market price.\\
		
		Previous studies have shown that quantitative technical analysis can be effective when used by an automated agent to predict market direction on financial markets\cite{schoreels2004agent}. In Technical analysis of the financial markets: A comprehensive guide to trading methods and applications\cite{murphy1999technical} John Murphy states that "One of the great strengths of technical analysis is its adaptability to virtually any trading medium and time dimension". The traditional gamblers approach is to look at the fundamentals surrounding a sporting event, i.e. the previous form of a horse or who the jockey is. This section of the report will attempt to use machine learning and quantitative technical analysis techniques to train a model from historical Betfair price data to predict market trends in real time.
		
	\section{Historical Market Data}
		The decision was made to focus this part of the project on horse racing data specifically the win markets. Betfair horse racing markets have some of the highest liquidity levels on the exchange, which helps to stabilise price movement. Markets with lower liquidity levels can exibit eractic price movement because orders can have a larger effect on the price as they strip out the small levels of volume. Horse racing markets are especially liquid just before the event and there is a large community of people trading these pre-race markets on Betfair.\cite{coldTradingOnBetfair}\cite{googleSearchHorseRacing}\\
		
		Betfair and their third part data provider Fracsoft kindly provided me with one months historical tick data for horse racing markets from the 11th October 2014 to the 10th November 2014. This data was generated by polling the Betfair exchange four times a second, producing a record every time there was a delta in the market data.\\
				
	\section{Preprocessing the Data}
		The months data comprised of 97,422,530 records spread over 848 markets (races), comprising of 8358 selections  (runners). The data was read into a MongoDB database, splitting the data into collections, one for each market.	\\
	
		To make the price data easier to analyse the data for each market would be grouped into a series of intervals, each interval representing a period of time of the same length. Each interval will have the high, low and closing price and the volume traded for that period. The aim, once a set of intervals had been created for each market was to apply technical analysis indicators to the intervals. These indicators would constitute the features used by a machine learning model to classify if the next interval in the series was going to close above (up), below (down), or at the same level (none) as the current intervals closing price.\\
		
		Two sets of intervals were built for each market, one set comprising of 1 minute intervals and the other of 10 second intervals, with a view to compared a models accuracy over both time series.\\ 
		
		Finally each interval was labelled as "UP", "DOWN" or "NONE" according to the delta in closing price compared to the closing price of the next interval in the series.
		
		\subsection{Betfair Price Data}
			\begin{table}[h]
				\centering
				\begin{tabular}{S[table-format=4.2]
								S[table-format=2.1]
								S[table-format=2.1]}
					\toprule
					{Decimal Odds} 	& \multicolumn{2}{c}{Implied Probability (\%)}\\
					{}				& {Win}		& {Lose}	\\
					\midrule
					1.01				& 99 		& 1 \\
					1.50				& 66.6		& 33.3 \\				
					2.00 			& 50			& 50 \\
					3.00				& 33.3		& 66.6 \\				
					100.00			& 1			& 99 \\				
				\end{tabular}
				\caption{Implied Probability of Decimal Odds}
				\label{tab:impliedProbabilityOfDecimalOdds}
			\end{table}
		
			Betfair prices are represented as decimal odds. Table \ref{tab:impliedProbabilityOfDecimalOdds} shows the implied probabilities of a series of decimal odds and you can see that the price range between an implied probability of winning of 99\% and 50\% is 0.99 and the price range between 50\% and 1\% is 98. 
			
		\todo[inline]{further analysis needs to be carried out to see if converting the prices into applied probabilities has any effect on the classification result, there may be a bug in here !}
				
		\subsection{Technical Analysis Indicators}
				Once the intervals had been produced and the prices normalised the technical indicators documented in the appendix were applied to the intervals.
			
			\todo[inline]{record indicators in the appendix}
						
			All the indicators used in this project are widely used in financial trading and they were selected because their output values lie within a predefined range and the output can therefore be compared across markets. One major difference between financial markets and horse racing markets is that a horse racing market has a much shorter life span than that of a financial market. Models trained on financial markets are usually trained using data from the same market they will be used to predict. This will not be possible with the horse racing markets as there isn't enough data. The model in this project will be trained using the labelled intervals produced over all the markets in the historical data provided. It is therefore import that the indicators used can be compared across markets.\\
			
			Each indicator was written in Scala and their implemetations tested by downloading example data from the Chart School website\cite{technicalIndicators} and creating unit tests that asserted given the input in the example data the implementation of each indicator produced the same output as that in the example data.\\ 
			  
			On its own each of these indicators is unlikely to accurately predict or confirm price movement in all the markets, all the time, but it is hoped by using a combination of indicators the model will be able to see past any noise in the data.\\			
		
	\section{Choosing a Model}
		The decision was made to use a Random Forest to classify the data. A Random Forest is made up of an ensemble of decision trees and was initially proposed in the 2001 paper Random Forests by Leo Breiman\cite{breiman2001random}. Decision trees are expressive and easy to understand and implement due to their recursive nature\cite{Flach:2012:MLA:2490546} and this greatly influenced the decision.\\ 
		
		% Can handle multi dimensional data
		% Expressive and easy to understand
		% Easy to impement due to their recursive nature
		% Find study comparing Random Forest to other classifiers
		
			\begin{figure}[H]
				\includegraphics[width=8cm]{"Predicting Price Movement/Images/Decision Tree for Mammal Classification".png}
				\centering
				\caption{Decision Tree for Mammal Classification}
    				\label{fig:decisionTree}
			\end{figure}		
								
		Decision Trees work by splitting the data, one feature at a time, using the feature that produces the best split of the training data at that given node until the data is of a single classification or a set size. The quality of the split is commonly decided by maximising the entropy gain\cite{entropy} but other measures can be used. Figure~\ref{fig:decisionTree} shows an example of a simple decision tree for classifiying mammals. Decision Trees are however prone to over-fitting as the model can to closely learn the training data and as such the model usually displays low bias and high variance. One solution to this problem is to prune the decision tree which is the approach that the C4.5 Decision tree algorithm takes\cite{quinlan2014c4}.\\
	
		\subsection{Random Forest}
			The Random Forest model proposed by Breiman solves the problem of over-fitting by growing an ensemble of trees and letting them vote for the most popular class. Each tree is grown using a subset of the training data the same size as the training data but selected at random from the initial dataset using replacement, these subsets are known as bootstrap samples. As each tree is grown the algorithm only uses a random subset of features at each split to split on, this is known as subspace sampling. Introducing this randomness into the trees encourages diversity within the forest and reduces the tendency to over fit the data, increasing the bias and decreasing the variance. Trees are full grown without pruning until, all the instances in a node are of the same class or all the features in a node are of the same value or the number of instances in a node is less than or equal to a predefined number. This results in three options for the forest that can optimised during the training process: the number of trees in the forest, the number of random features to choose from at each split and the number maximum number of instances in a leaf node.\\					
									
	\section{Implementation}
		It was initially considered that the model be implemented using a tool-kit. However the only toolkit found that included a Random Forest was Apache Sparks MLlib\cite{spark}, using this tool-kit required also using Apache Spark and submitting jobs to a Spark cluster. At this stage of the project introducing Spark to the list of technologies used would require a lot of time and greatly increase complexity. As such the decision was made to implement the Random Forest from scratch in Scala.\\
		
		\todo[inline]{include table of training \& testing data set sizes here}.		
		
		Once the data had been processed and labelled and the Random Forest proposed in Breiman's paper was implemented the data was split into two sections. Data from 11th October 2014 to 31st October 2014 was to be used for training and data from 1st November 2014 to 10th November 2014 was to be used for testing.\\
		
		Unfortunately training the model took a very long time. The majority of the time was spent selecting the splits as the trees were grown because all of the features in the data were continuous, rather than belonging to a finite number of categories. For a dataset of $n$ instances $n - 1$ splits would have to considered for each feature. If three randomly picked features were considered for each split this would mean $3 * (n - 1)$ splits being compared.\\ 
		
		One solution might have been to profile the JVM whilst the model was being trained to see if it could be optimised to run faster but instead a revised version of the algorithm was used.
		\subsection{Extremely Random Forest}
			Extremely Random Forests differ from Random Forests in two major ways and have, in some instances, been shown to produce better results\cite{geurts2006extremely}:
			
			\begin{enumerate}
				\item Each tree is trained using the whole of the training data, not a sub-sample.
				\item As the trees are grown the splits are picked at random. If there were 3 random features being considered at each split a random value would be selected for each feature within the range that feature had in the dataset being split. The resulting entropy of splitting on the 3 features would then be compared using these random values for the split and the one that produced the best split chosen.
			\end{enumerate}
			
			This algorithm removes the randomness introduced in Breiman's algorithm by sub-sampling the data set used for each tree but then introduces randomness by picking the value to split on at random. Selecting a value at random is computationally faster than comparing all the possible splits and will scale better as a random number can be picked in constant time and as such will not be effected as the size of the data increases.\\
		
		The algorithm to grow the trees used in the Extremely Random Forest is show in \ref{fig:extremelyRandomForestTree} and has been taken from Extremely randomized trees: Pierre Geurts, Damien Ernstm, Louis Wehenkel\cite{geurts2006extremely}
		
			\begin{figure}[H]
				\includegraphics[width=12cm]{"Predicting Price Movement/Images/Extremely Random Forest".png}
				\centering
				\caption{Algorithm for growing a Decision Tree for an Extremely Random Forest}
    				\label{fig:extremelyRandomForestTree}
			\end{figure}		
		
	\section{Optimisation}				
		Once implemented the forest was trained and the number of trees, number of features to split on and leaf size optimised using cross-validation on the training set. The result can be seen below
		
		\todo[inline]{Add optimisation results once analysis on converting the price data has been completed}
		
	\section{Results}
		Once the optimised the following results were achieved training the classifier with the settings selected during optimisation.
	
		\todo[inline]{
		I will add the results before the final draft, I was thinking I could test for bugs in the classifier by loading the data into python and testing it using the sklearn toolkit. If the results are very different I will know there is a bug in my software
		}	
	
	\section{Integration With The Trading System}
		The trained model will be serialised to mongoDB and made available to use by the auto trader 	
	
		\todo[inline]{this section to be added before the final draft}