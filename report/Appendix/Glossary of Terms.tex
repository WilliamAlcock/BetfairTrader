	\chapter{Glossary of Terms}
	\label{appendix:glossaryOfTerms}
	
		\section{Gambling Terms}
	
		\subsubsection{Back}
		The term Back is used on betting exchanges and means that you bet ON a result happening. Placing a Back bet is the same as placing a bet with a normal book maker.
	
		\subsubsection{Lay}
		The term Lay is used on betting exchanges and means that you bet AGAINST a result happening. When placing a Lay bet you are taking the liability of the bet, in effect you are taking the role of a normal book maker.
		
		\subsubsection{Side of a Bet}
		Either Back or Lay.

		\subsubsection{Offering Odds}
		Placing a lay bet at a given price.
	
		\subsubsection{Stake/Size}
		The amount the bet is for.

		\subsubsection{Odds/Price}
		The amount returned, in respect to the stake, if a result happens. All prices on the Betfair Exchange and in this dissertation are displayed as decimal odds.

		\subsubsection{Decimal odds vs Fractional odds}
		Traditionally bookmakers in the United Kingdom have displayed prices as fractional odds, for example:\\

		If the fractional odds are $3/1$ and you place a back bet of $\pounds10$ and you win, your total return would be: 
		$$\pounds10 \times 3\ as\ profit + your\ original\ \pounds10\ stake\ = \pounds40$$

		If the decimal odds are $4.0$ and you place a back bet of $\pounds10$ and you win, your total return would be: 
		$$\pounds10 \times 4.0 = \pounds40$$

		Decimal odds can be easier to calculate than fractional odds with a denominator that is not 1, for example:\\

		The fractional odds $6/5$ are the equivalent of the decimal odds $2.2$\\

		The total return if you place a back bet of $\pounds10$ at odds of $2.2$ and you win would be: 
		$$\pounds10 \times 2.2 = \pounds22$$ 
		
		This is easier than calculating:
		$$\Big(\pounds10 \times \frac{6}{5}\Big) + \pounds10 = \pounds22$$

		The important thing to remember is decimal odds always include the unit stake thus every price will be greater than 1. Fractional odds represent the profit, $6/5$ means you will win six pounds for every five pounds staked. 

		\subsubsection{Profit}
		The amount you could profit
			\begin{itemize}
				\item For a Back bet this is calculated as $(price \times size) - 1$
				\item For a Lay bet this is the size of the bet
			\end{itemize}

		\subsubsection{Liability}
		The amount you could lose
			\begin{itemize}
				\item For a Back bet this is the size of the bet you have placed.
				\item For a Lay bet it is the amount it will cost you if the selection wins, this is calculated as $(price \times size) - 1$  			
			\end{itemize}
			
		\subsubsection{Return}
		Profit - Liability
			
		\subsubsection{Matched Bets}
		Bets that have not yet traded as they have not been paired with a bet of the same price from the opposite side.
				
		\subsubsection{Unmatched Bets}
		Bets that have been paired with a bet of the same price from the opposite side.
		
		\subsubsection{Orders}
		Bets
		
		\section{Machine Learning Terms}
		
		\subsubsection{Entropy}
		A way to measure (im)purity.\\
		
		$$ Entropy = \sum_{x} -p_x \log_2 p_x$$	
		
		Where p is the probability of the class.
		
		For Example:\\
		
		\begin{table}[h]
			\begin{tabular}{l}
				Class 			\\
				\toprule
				Red\\
				Red\\
				Red\\
				Red\\
				Red\\
				Green\\
				Green\\
				Yellow\\
				Yellow\\
				Yellow\\
			\end{tabular}
		\end{table}	
		
		The probability of each colour in the set above is:
		\begin{align*}
			\text{Probability(Red)} &= \frac{5}{10} = 0.5\\
			\text{Probability(Green)} &= \frac{2}{10} = 0.2\\
			\text{Probability(Yellow)} &= \frac{3}{10} = 0.3
		\end{align*}		 
		So the entropy of the set is:
					
		$$Entropy = - 0.5\ log_2(0.5) - 0.2\ log_2(0.2) - 0.3\ log_2(0.3) = 1.485$$
		
		Entropy reaches its maximum when all the classes have equal probability. The entropy of a set with only one class is zero.
		
		\subsubsection{Overfitting}
		Overfitting occurs when a model is too closely fit to the training data and does not generalise over the data. The line on right graph in Figure~\ref{fig:overfitting} over fits the data. This causes a problem because the training data is only a sample of the problem space and will contain noise. If a model overfits the training data it will have a low error when used to predict the training data but a high error when used to predict data outside of the training data.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.65\linewidth]{"Appendix/Images/Overfitting".png}
			\caption{Example of underfitting, rightfitting and overfitting\cite{stackExchangeOverfitting}}
    			\label{fig:overfitting}
		\end{figure}	
		
		\subsubsection{Bias}
		Represents the ability of a model to approximate the data. A model that underfits would display high bias, a model that overfits would display low bias.
		
		\subsubsection{Variance}
		The sensitivity of a model to changes in the data points. If a model is retrained the variance is how much the model changes when the data points change.
				
		\subsubsection{Sampling with Replacement Vs Sampling without Replacement}
		Consider the set of numbers [1, 2, 3]
		
		\begin{itemize}
			\item Sampling without replacement - if two numbers are selected from the set without replacement the possible combinations are: [1, 2], [1, 3], [2, 3], [2, 1], [3, 1], [3, 2] because the first number is removed from the set before selecting the second number.
			\item Sampling with replacement - if you select two numbers from the set with replacement the possible combinations are: [1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3], [3, 1], [3, 2], [3, 3] because the first number is replaced in the set making it available for the second selection.
		\end{itemize}
		
		In sampling with replacement the sample values are independent, when sampling without replacement they are not.
				
		\subsubsection{Pruning a Decision Tree}
		Pruning a decision tree removes parts of a decision tree that have less effect on classification, reducing its complexity. Pruning is used to increase a trees accuracy by reducing overfitting.