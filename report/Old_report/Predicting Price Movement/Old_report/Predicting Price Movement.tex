\chapter{Predicting Price Movement}
	\section{Technical Analysis}
		There are two main schools of thought in financial markets, technical analysis and fundamental analysis. Technical analysis concentrates on the study of market action by looking a the price previous price movement of a security, fundamental analysis focuses on the economic forces of supply and demand that cause prices to move higher, lower, or stay the same\cite{murphy1999technical}. Both approaches have the same desired outcome: to predict the direction of future price movement. Technical Analysis assumes that the economic forces of supply and demand are already implied by the market price.\\
		
		Previous studies have shown that quantitative technical analysis can be effective when used by an automated agent to predict market direction on financial markets\cite{schoreels2004agent}. In Technical analysis of the financial markets: A comprehensive guide to trading methods and applications\cite{murphy1999technical} John Murphy states that "One of the great strengths of technical analysis is its adaptability to virtually any trading medium and time dimension". The traditional gamblers approach is to look at the fundamentals surrounding a sporting event, i.e. the previous form of a horse or who the jockey is. This section of the report will attempt to use machine learning and technical analysis to train a model from historical Betfair price data to predict market trends.
		
	\section{Historical Market Data}
		The decision was made to focus this part of the project on horse racing data, specifically the win markets. Betfair horse racing markets have some of the highest liquidity levels on the exchange, which helps to stabilise price movement. Markets with lower liquidity levels can exibit eractic price movement because orders can have a larger effect on the price as they strip out the small levels of volume. Horse racing markets are especially liquid in the 20 minutes before the event and there is a large community of people trading these pre-race markets on Betfair.\cite{coldTradingOnBetfair}\cite{googleSearchHorseRacing}\\
		
		Betfair and their third part data provider Fracsoft kindly provided me with one months historical tick data for horse racing markets from the 11th October 2014 to the 10th November 2014. This data was generated by polling the Betfair exchange four times a second, producing a record every time there was a delta in the market data.
				
	\section{Preprocessing the Data}
		The months comprised of 97,422,530 records spread over 848 markets (races), comprising of 8358 selections (runners). The data was read into a MongoDB database, splitting the data into collections, one for each market.\\
	
		\subsection{Grouping The Data Into Intervals}
			The data for each market was grouped into a series of 1 minute intervals. For each interval the high, low and closing price and the volume traded were recorded.\\
			
			The aim, once the intervals had been created for each market, was to apply technical analysis indicators to the intervals which would constitute the features used by a machine learning model to classify if the closing price of the next interval in the series was going to be above (up), below (down), or at the same (none) as the current interval's closing price.
		
		\subsection{Converting Prices to Implied Probabilities}
			\begin{table}[h]
				\centering
				\begin{tabular}{S[table-format=3.2]
								S[table-format=2.1]
								S[table-format=2.1]}
					\toprule
					{Decimal Odds} 	& \multicolumn{2}{c}{Implied Probability (\%)}\\
					{}				& {Win}		& {Lose}	\\
					\midrule
					1.01				& 99.0 		& 1.0 \\
					1.50				& 66.6		& 33.3 \\				
					2.00 			& 50	.0		& 50.0 \\
					3.00				& 33.3		& 66.6 \\				
					100.00			& 1.0		& 99.0 \\				
				\end{tabular}
				\caption{Decimal Odds vs Implied Probability}
				\label{tab:impliedProbabilityOfDecimalOdds}
			\end{table}
		
			Betfair prices are represented as decimal odds but prices on financial markets are displayed in pence (for securities traded in sterling). Table \ref{tab:impliedProbabilityOfDecimalOdds} shows the implied probabilities of a series of decimal odds. The prices were converted to implied probabilities so that an equal weighting would be given to price movements between 1.01 and 2.00 as price movements between 2.00 and 100.00.
							
		\subsection{Technical Analysis Indicators}
			Once the data had been grouped into intervals and the prices converted to implied probabilities the technical indicators documented in the appendix were applied to the intervals.\\
			
			\todo[inline]{record indicators in the appendix}
						
			The indicators used in this project are all widely used in financial trading and they were selected because their output values lie within a predefined range and the output can therefore be compared across markets. One major difference between financial markets and horse racing markets is that a horse racing market has a much shorter life span than that of a financial market. Models trained on financial markets are usually trained using data from the same market they will be used to predict. This will not be possible with the horse racing markets as there isn't enough data. The model in this project will be trained using the labelled intervals produced over all the markets in the historical data provided. It is therefore import that the indicators used can be compared across markets.\\
			
			Each indicator was written in Scala and their implementations tested by downloading example data from the Chart School website\cite{technicalIndicators} and creating unit tests that asserted given the input in the example data the implementation of each indicator produced the same output as that in the example data.\\ 
			  
			On its own each of these indicators is unlikely to accurately predict or confirm price movement in all markets, all the time, but it is hoped by using a combination of indicators the model will be able predict the trend.		
		
	\section{Choosing a Model}
		It was decided that a Random Forest would be used to classify the data. A Random Forest was selected because:
		
		\begin{itemize}
			\item They have shown excellent performance for both classification and regression problems.
			\item They are expressive and easy to understand\cite{Flach:2012:MLA:2490546}.
			\item They are easy to implement due to their recursive nature\cite{Flach:2012:MLA:2490546}.
			\item They can handle multiple output problems.
		\end{itemize}		 
		
		A Random Forests is made up of an ensemble of decision trees each built from samples of the training data drawn with replacement and grown using random subsets of features. Random Forests were initially proposed in the 2001 paper Random Forests by Leo Breiman\cite{breiman2001random}. 
			
		\subsection{Decision Trees}			
				
		\begin{figure}[H]
			\includegraphics[width=8cm]{"Predicting Price Movement/Images/Decision Tree for Mammal Classification".png}
			\centering
			\caption{Decision Tree for Mammal Classification}
    			\label{fig:decisionTree}
		\end{figure}		
								
		Decision Trees classify data by learning simple decision rules inferred from the data. Decision Trees are grown by splitting the data, one feature at a time, selecting the feature that produces the best split of the data. This procedure is repeated until the data is of a single classification or a set size. The quality of the split is commonly decided by maximising the entropy gain\cite{entropy} but other measures can be used. Figure~\ref{fig:decisionTree} shows an example of a simple decision tree for classifiying mammals.\\
		
		Advantages of decision trees:
			\begin{itemize}
				\item Simple to understand.
				\item Able to handle both numerical and categorical data.
				\item Able to handle multiple output problems.
				\item Easy to interpret as the explanation can be explained using boolean logic (in contrast to a neural network)
			\end{itemize}					
		
		Disadvantages of decision trees:
			\begin{itemize}
				\item Can be prone to over-fitting as they do not generalise well and as such exhibit low bias. One solution to this problem is to prune the tree which is the approach that the C4.5 Decision tree algorithm takes\cite{quinlan2014c4}.
				\item Can exhibit high variance as small changes in the data can result in a completely different model.
			\end{itemize}					

		\subsection{Random Forest}
					
		A Random Forest is an ensemble of decision trees classifies data by taking the most popular class as voted for by the each tree.\\
		
		Each tree is grown using a subset of the training data the same size as the training data but selected at random from the initial dataset using replacement, these subsets are known as bootstrap samples. As each tree is grown the algorithm only uses a random subset of features at each split to split on, this is known as subspace sampling. Trees are full grown without pruning until, all the instances in a node are of the same class or all the features in a node are of the same value or the number of instances in a node is less than a predefined number. This results in three configurable options that can optimised during the training process: 

		\begin{enumerate}
			\item The number of trees in the forest
			\item The number of random features to choose from at each split
			\item The minimum number of instances to split on.
		\end{enumerate}				
		
		Introducing randomness into the trees, through the bootstrap samples and subspace sampling, encourages diversity within the forest and as a result the bias of the forest is usually slightly higher than that of a single non-random decision tree but the variance also decreases which, in most cases, more than compensates for the increase in bias.\\

		Unfortunately there is significant computational overhead when training a Random Forest for the data such as that in this study. This is because all the features of the data have continuous values rather than ones that fit into a finite number of categories. At each split of each tree all the possible splits of the data data for each feature need to be analysed to see which provides the best fit. For a dataset of $n$ instances $n - 1$ splits would need to considered for each feature (assuming all the values were unique). If three randomly picked features were considered this would result in $3 * (n - 1)$ comparisons for each split. In order to decrease this overhead a variation on the Random Forest was finally selected.
		
		\subsection{Extremely Random Forest}
		
		Extremely Random Forests\cite{geurts2006extremely} build on the Random Forests algorithm by introducing an extra level of randomness. As each tree is grown the thresholds at which each feature is split on are picked at random. If there were 3 random features being considered at each split a random value would be selected for each feature within the range of that feature in the dataset being split. The resulting entropy of splitting on the 3 features will be compared using these random values and the one that produces the best split chosen.\\
			
			Selecting a value at random is computationally faster than comparing all the possible splits and therefore scales better as a random number can be picked in constant time will not be effected as the size of the data increases.\\
		
		The algorithm to grow the trees used in the Extremely Random Forest is show in \ref{fig:extremelyRandomForestTree} and has been taken from Extremely randomized trees: Pierre Geurts, Damien Ernstm, Louis Wehenkel\cite{geurts2006extremely}
		
			\begin{figure}[H]
				\includegraphics[width=12cm]{"Predicting Price Movement/Images/Extremely Random Forest".png}
				\centering
				\caption{Algorithm for growing a Decision Tree for an Extremely Random Forest}
    				\label{fig:extremelyRandomForestTree}
			\end{figure}		
 
	\section{Implementation}
		It was initially considered that the model be implemented using a tool-kit. However the only toolkit found that included a Random Forest was Apache Sparks MLlib\cite{spark}. This tool-kit can only be used with Apache Spark and jobs must be submitted to a Spark cluster. At this stage of the project introducing Spark to the list of technologies used would require a lot of time and greatly increase complexity. Also Spark's MLlib does not include an implementation for Extremely Random Trees.\\ 
		
		Using a model implemented in Python's SkLearn library\cite{Sklearn} and calling it from Scala was also considered but rejected due to the overhead of the infrastructure involved in communicating between Scala and Python\cite{CallingPythonFromScala}. As such the decision was made to implement the Extremely Random Forest from scratch in Scala.\\
			
		% over sampling to ensure equal distribution in the training data for each tree
		% optimisation
			% graphs & tables
		% results
		% comparison using the implementation in sklearn
		% weak features
		
		% model written to mongodb as json to be loaded my the server
		
		Once the data had been grouped into intervals, the prices converted and the technical indicators applied to each interval each interval was labelled as one of three classes: "Up", "Down" or "None" indicating the delta between the interval's close and the close of the next interval in the series.\\
		
		The data was then split into sets: training and testing. The training set comprised of data from markets between the 11th October 2014 and the 31st October 2014. The testing set comprised of data from the 1st November 2014 to the 10th November 2014. As stated before the majority of the liquidity in the horse racing markets on the Betfair exchange is introduced during the last 20 minutes before the start of a race, without sufficient liquidity prices could move irrationally. Because of this the decision was made to only include intervals from the last 20 minutes of trading for each race in both the training and testing sets with a view to only predict price movements in the last 20 minutes of trading on any market. It should be pointed out that the indicators were applied on the intervals \textit{before} this was done as the majority of them are averaged over more than 20 data points and as such would require more than 20 minutes of data to output useful values.

		\begin{table}[h]
			\centering
			\begin{tabular}{lrr}
				\toprule
				{Class}		& \multicolumn{2}{c}{Number of Instances}\\
				{}			& \multicolumn{1}{c}{Training Set} & \multicolumn{1}{c}{Testing Set}\\							
				\midrule
				{Up} 		& 36780		& 14918\\
				{Down}		& 35117		& 13571\\
				{None}		& 42198		& 14907\\
				\bottomrule
				{}			& 114095		& 43396\\
			\end{tabular}
			\caption{Training and Testing Datasets}
			\label{tab:trainingAndTestingDatasets}
		\end{table}		
		
		The number of instances by class for each dataset is shown in Figure~\ref{tab:trainingAndTestingDatasets}. As you can see the training data cannot be split evenly by class and as such is imbalanced. Even if this training set was balanced there would be no  guarantee that future training sets would be. Imbalanced training data can lead to a model that favours the most frequently occurring classes in the training data.\\
		
		There are two common approaches to tackle this problem: the first is to assign a cost to misclassification, with a higher cost associated with the minority class whilst trying to minimize the overall cost. The second is to either down-sample the majority class or over-sample the minority class when creating each bootstrapped dataset\cite{chen2004using}. The second of these approaches was used in this implementation. The original dataset was divided into sets by class and each of the resulting sets sampled with replacement so that resulting bootstrapped set was the same size as the original dataset and had an equal distribution of instances for each class. This method could result in majority classes being over-sampled and minority ones being down-sampled depending on the original dataset.\\
		
		The trees were then fully grown using the bootstrapped datasets until one of the following conditions was met:
		
		\begin{itemize}
			\item The number of instances is less than the min leaf size to split
			\item All the labels are of the same classification
			\item All the attributes are constant
		\end{itemize}
		
		Because this is a classification problem as opposed to a regression problem the min number of instances to split will be set to 2 so the leaves only contain instances of the same class. If a leaf does contain instances of more than one class, which could occur if all the attributes were constant, the implementation will select one of those classes at random.\\
						
	\section{Optimisation}
		Once implemented the number of features to split on and the number of trees in the forest were optimised to reduce to out-of-bag error over the training data.
		
		\subsection{Out-Of-Bag Error}
			Each tree in the forest is grown using a bootstrapped sample. When this sample is created about 37\% of the original dataset is left-out, this is known as the out-of-bag sample. Once the tree has been grown it is used to classify each instance in the out-of-bag sample, after all the trees have been grown these votes are aggregated and used to classify all the instances in the original dataset. As such the out-of-bag error rates is:
			
			$$\frac{number\ of\ mis-classified\ instances}{total\ number\ of\ instances}$$ 

This out-of-bag error has been shown to be as accurate as using a test set the same size as the training set\cite{breiman1996out}.\\ 

		\subsection{Optimisation Results}
		The results in Tables~\ref{fig:optimisationResultsTables} Figure~\ref{fig:optimisationResultsGraphs} were recorded using the default settings below changing either the number of features or the number of trees.

	\begin{table}[h]
		\begin{tabular}{ll}
			{number of trees} 				& 100\\
			{minimum leaf size to split}		& 2\\
			{number of features to split on}	& 3\\
		\end{tabular}
	\end{table}		
		
		As shown in Table~\ref{fig:outOfBagErrorFeaturesTable} Figure~\ref{fig:outOfBagErrorFeaturesGraph} the optimal setting for the  number of features to split on in respect to minimising the error rate was 4 which coincides that recommended by Brieman, the square root of the total number of features.\\
		
		As shown in Table~\ref{fig:outOfBagErrorTreesTable} Figure~\ref{fig:outOfBagErrorTreesGraph} the error rate decreases as the number of trees in the forest increases.

	\begin{table}[H]
		\centering
		\begin{subtable}{.45\linewidth}\centering
			\hfill
			\caption{error rate for features}
			\label{fig:outOfBagErrorFeaturesTable}
			\pgfplotstabletypeset[
    				col sep=comma,
	    			string type,
    				columns/features/.style={column name=Features, column type={l}},
    				columns/error_rate/.style={column name=Error Rate, numeric type,precision=6,zerofill,dec sep align},
	    			every head row/.style={before row=\toprule,after row=\midrule},
    				every last row/.style={after row=\bottomrule},
    			]{Predicting Price Movement/Results/features_error_rate.csv}
    		\end{subtable}
    		\hfill
		\begin{subtable}{.45\linewidth}\centering
			\caption{error rate for number of trees}
			\label{fig:outOfBagErrorTreesTable}
			\pgfplotstabletypeset[
    				col sep=comma,
    				string type,
    				columns/trees/.style={column name=Trees, column type={l}},
    				columns/error_rate/.style={column name=Error Rate, numeric type,precision=6,zerofill,dec sep align},
    				every head row/.style={before row=\toprule,after row=\midrule},
    				every last row/.style={after row=\bottomrule},
    			]{Predicting Price Movement/Results/trees_error_rate.csv}
    		\end{subtable}
    		\caption{Tables of Optimisation Results}
    		\label{fig:optimisationResultsTables}	
    	\end{table}
		
	\begin{figure}[H]
		\begin{subfigure}{.47\linewidth}\centering
			\caption{error rate for features}
			\label{fig:outOfBagErrorFeaturesGraph}
			\begin{tikzpicture}
				\begin{axis}[xlabel={number of features}, ylabel={error rate}, y tick label style={/pgf/number format/.cd, fixed, fixed zerofill, precision=3, /tikz/.cd}]
					\addplot table [x=features, y=error_rate, col sep=comma]{Predicting Price Movement/Results/features_error_rate.csv};
				\end{axis}
			\end{tikzpicture}
		\end{subfigure}%
		\hfill
		\begin{subfigure}{.47\linewidth}\centering
			\caption{error rate for number of trees}
			\label{fig:outOfBagErrorTreesGraph}
			\begin{tikzpicture}
				\begin{axis}[xlabel={number of trees}, ylabel={error rate}, y tick label style={/pgf/number format/.cd, fixed, fixed zerofill, precision=3, /tikz/.cd}]
					\addplot table [x=trees, y=error_rate, col sep=comma]{Predicting Price Movement/Results/trees_error_rate.csv};
				\end{axis}		
			\end{tikzpicture}
		\end{subfigure}
		\hfill
		\caption{Graphs of Optimisation Results}
    		\label{fig:optimisationResultsGraphs}	
	\end{figure}
				
	\section{Testing Results}
		The following results were produced training the classifier on the training set with the following settings selected during the optimisation stage and testing the resulting model on the testing set:
		
	\begin{table}[h]
		\begin{tabular}{ll}
			{number of trees} 				& 1000\\
			{minimum leaf size to split}		& 2\\
			{number of features to split on}	& 3\\
		\end{tabular}
	\end{table}
		
	\begin{table}
		\begin{tabular}{ll|S[table-format=5] S[table-format=5] S[table-format=5]| S[table-format=5]}
			\toprule
			\multicolumn{2}{c}{} 					& \multicolumn{3}{|c|}{Predicted}	& {}\\
			{}		& {}		 						& {Up} 				& {Down} 	& {None} 	& {}\\
			\midrule
			\multirow{ 3}{*}{Actual} 	& {Up}		& \color{red}6674	& 5254				& 2990				& \textbf{14918}\\
			{}							& {Down}		& 4811				& \color{red}6377	& 2383				& \textbf{13571}\\
			{}							& {None}		& 5106				& 4895				& \color{red}4906	& \textbf{14907}\\
			\midrule
			{} 							& {}			& \textbf{16591}		& \textbf{16526}		& \textbf{10279}		& \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{table}
		\pgfplotstabletypeset[
    			col sep=comma,
	    		string type,
    			columns/class/.style={column name=Class, column type={l|}},
    			columns/distribution/.style={column name=Distribution, numeric type,precision=6,zerofill,dec sep align},
    			columns/precision/.style={column name=Precision, numeric type,precision=6,zerofill,dec sep align},
    			columns/recall/.style={column name=Recall, numeric type,precision=6,zerofill,dec sep align},
	    		every head row/.style={before row=\toprule,after row=\midrule},
    			every last row/.style={after row=\bottomrule},
    		]{Predicting Price Movement/Results/results.csv}
	\end{table}	
	
	\begin{table}	
		\begin{tabular}{lS[table-format=1.6]}		
		Out-of-bag error 			& 0.597265\\	
		Accuracy			 			& 0.413794\\ 
		Weighted Average Precision	& 0.422910\\
		Weighted Average Recall		& 0.413794\\
		\end{tabular}
	\end{table}